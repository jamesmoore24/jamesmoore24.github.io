import Header from "../../../components/Header";
import Link from "next/link";
import Image from "next/image";

export default function ModelOptimizationPost() {
  return (
    <div className="min-h-screen bg-green-50 flex flex-col">
      <Header />

      <main className="flex-1 pt-32 pb-16 px-8 max-w-4xl">
        <div className="font-noto-serif text-gray-800 space-y-8">
          <Link
            href="/thoughts"
            className="text-gray-600 hover:text-gray-800 transition-colors text-sm"
          >
            ← back to thoughts
          </Link>

          <article className="space-y-8">
            <header className="space-y-4">
              <h1 className="text-4xl font-medium text-gray-800">
                how to make models run faster
              </h1>
              <time className="text-sm text-gray-500 italic block">
                December 2025
              </time>
            </header>

            <div className="prose prose-gray max-w-none space-y-6">
              <p className="leading-relaxed text-gray-700">
                As part of my work at Meta I was given the (sorta) vague task
                of &quot;improve this language identification model please.&quot; at
                first i was pretty overwhelmed since I had never touched a
                production model let alone a model that was queried as much as
                this one across the company but (as any over-ambitious new
                grad) I was determined to make it happen. Long story short i
                was able to fix some pretty serious problems regarding english
                recall with the model which basically meant that obviously
                english text wasn&apos;t being classified as some other language
                anymore which is really good!
              </p>

              <p className="leading-relaxed text-gray-700">
                However there was another issue and one that I think is
                probably equally as important as fixing critical model quality
                issues. How do we actually transfer these quality improvements
                to production? As is normal at Meta we were deploying on a
                new internal framework so we were completely unaware of
                any potential challenges that we could face. We sort of thought
                that since our improved model had the roughly the same number
                of parameters and architecture as our previous model that we
                would automatically have the same throughput in production. We
                were wrong about that (lol). We had roughly 2.5x throughput
                than our previous deployment which meant that we couldn&apos;t
                deploy our improvements at all!
              </p>

              <p className="leading-relaxed text-gray-700">
                If you don&apos;t know what throughput means, what determines it and
                why it&apos;s super-duper important to benchmarking performance here&apos;s a quick explanation:
              </p>

              <p className="leading-relaxed text-gray-700">
                Imagine your waiting in some line for pizza. Now what would you
                expect that the time it takes to go from entering the store to
                actually getting the slice of pepperoni pizza (the best kind of
                pizza) would depends on? Hmmm… you&apos;d probably guess it depends
                on the number of people in line, the number of cashiers taking
                orders, how long each order takes, the number of chefs the
                number of ovens, and then generally how fast each of the steps
                take.
              </p>

              <p className="leading-relaxed text-gray-700">
                We can define throughput (in this context) as the number of
                people that the pizza shop can serve over a given interval of
                time. We can also define E2E (end to end) latency as the time
                it takes for you to enter the shop to actually getting the
                pizza. Here&apos;s a neat diagram generated by NanoBanana pro which
                probably does a better job of describing the concept than I
                just did:
              </p>

              {/* TODO: Add diagram image here */}
              {<figure className="my-6">
                <div className="w-full max-w-3xl">
                  <Image
                    src="/throughput-pizza.png"
                    alt="Diagram illustrating throughput and E2E latency concepts"
                    width={1200}
                    height={675}
                    className="w-full h-auto rounded-md border border-gray-200"
                    priority
                  />
                </div>
                <figcaption className="mt-2 text-sm text-gray-500 text-center">
                  Throughput and latency visualization
                </figcaption>
              </figure>}

              <p className="leading-relaxed text-gray-700">
                So to translate, instead of a pizza shop we&apos;re dealing with
                production-grade machine learning systems using thousands of
                really expensive GPUs (aka the chefs) and trillions of requests
                (pizza orders aka HTTP request) and seeing how many requests we
                can process successfully over a period of around a second a.k.a
                QPS (queries per second).
              </p>

              <p className="leading-relaxed text-gray-700">
                Cool, back to the story. So now we&apos;re panicking because we
                can&apos;t actually deploy our model since we&apos;re regressing compared
                to our old model deployment. This is pretty bad since a lot of
                our clients don&apos;t have extra compute lying around to spend on a
                more &quot;expensive&quot; new model and thus we can&apos;t actually launch
                our improvements!
              </p>

              <p className="leading-relaxed text-gray-700">
                This is where I was tasked with trying to improve the QPS of
                our model back to where it was previously…. Let&apos;s do some model
                optimization to save the day:
              </p>

              <h2 className="text-2xl font-medium text-gray-800 mt-8 mb-2">
                Technique #1: profiling!
              </h2>
              <p className="leading-relaxed text-gray-700">
                How can we optimize something that we can&apos;t even see. It would
                be really nice to track where a single request goes in order to
                understand the different transformations it undergoes until it
                comes out the other end of our system as a language prediction!
                This is where profiling our CPU and GPU is really useful. The
                profiler is something that you run as you&apos;re running a QPS
                (load) test on a local GPU so you can see the actual activity
                that&apos;s happening. Understanding how to use the profiler is
                super important to improving your model. Here&apos;s an example of a
                what a profiler looks like:
              </p>

              <figure className="my-6">
                <div className="w-full max-w-3xl">
                  <Image
                    src="/gpu-trace.png"
                    alt="Perfetto (a popular profiling tool) GPU trace"
                    width={1200}
                    height={675}
                    className="w-full h-auto rounded-md border border-gray-200"
                  />
                </div>
                <figcaption className="mt-2 text-sm text-gray-500 text-center">
                  <a
                    href="https://modal.com/docs/examples/torch_profiling"
                    target="_blank"
                    rel="noopener noreferrer"
                    className="underline decoration-gray-400 hover:decoration-gray-600"
                  >
                    Perfetto (a popular profiling tool)
                  </a>{" "}
                  GPU trace
                </figcaption>
              </figure>

              <p className="leading-relaxed text-gray-700">
                What profiling is most useful for is understanding what is
                bottlenecking your system which can be used to generate
                hypotheses about what can improve your system&apos;s QPS the most.
                Thus, we can reduce model optimization efforts down to this
                (super generalized) iterative loop:
              </p>

              <ol className="list-decimal list-inside space-y-2 leading-relaxed text-gray-700 ml-4">
                <li>Profile model on standalone GPU during QPS load test</li>
                <li>Read profiler output to identify bottlenecks</li>
                <li>Optimize model deployment implementation</li>
                <li>Reprofile model and measure QPS</li>
                <li>If QPS increases that&apos;s good!! and if not… try again :)</li>
              </ol>

              <p className="leading-relaxed text-gray-700">
                If you want more details on GPU profiling I suggest you read
                this{" "}
                <a
                  href="https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  PyTorch documentation
                </a>{" "}
                as it goes in depth and is written by people that are better
                than me at this stuff :)
              </p>

              <h2 className="text-2xl font-medium text-gray-800 mt-8 mb-2">
                Technique #2: torch.compile
              </h2>
              <p className="leading-relaxed text-gray-700">
                One powerful model optimization technique is the idea of
                &quot;compiling&quot; your model. When I first heard of this I had only
                heard &quot;compilation&quot; in the context of programming languages. My
                memories flashed-back to how compilers do things like pruning
                syntax trees, unrolling loops and removing unnecessary
                branching so that the code we write is optimized to run on the
                machine we&apos;re using. In essence, torch.compile is trying to do
                the same thing except that the &quot;code&quot; is a PyTorch model
                defined by individual and nested modules and the machines we&apos;re
                running on are (usually) GPUs. This results in a model that has
                equivalent output but runs much faster.
              </p>

              <p className="leading-relaxed text-gray-700">
                Now how exactly does this happen and how do we do this? From a
                client&apos;s perspective it&apos;s easy! You just define some PyTorch
                model and then call:
              </p>

              <pre className="bg-gray-100 p-4 rounded-md border border-gray-200 overflow-x-auto">
                <code className="text-sm text-gray-800">
                  compiled_model = torch.compile(model)
                </code>
              </pre>

              <p className="leading-relaxed text-gray-700">
                Usually this works for most models since the actual operations
                which define most machine learning models are shared across
                various types of models. torch.compile is really good at
                spotting and taking advantage of these redundancies and
                (generally) replaces multiple instructions with kernels that
                are written to optimize the execution of those instructions on
                specific machines. How exactly to identify certain redundancies
                across models and write these optimized kernels is a (very
                difficult) problem that very talented people try to solve! More
                about how model compilation works is{" "}
                <a
                  href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  here
                </a>
                .
              </p>

              <p className="leading-relaxed text-gray-700">
                One quirk about using torch.compile is that it is NOT
                thread-safe. Why is this an issue? Going back to the pizza
                analogy now:
              </p>

              <p className="leading-relaxed text-gray-700">
                Imagine the Head Chef (the compiler) invents a brand new,
                super-fast way to make pizza and writes it in a master recipe
                book. Now, two chefs receiving different orders (pepperoni and
                vegetarian pizza) try to both optimize the recipe for their
                different orders. As one chef scribbles a change for a new
                order, the other chef unknowingly follows the halfway-written
                instruction and ruins their pizza.
              </p>

              {/* TODO: Add diagram image here */}
              {<figure className="my-6">
                <div className="w-full max-w-3xl">
                  <Image
                    src="/torch-compile-pizza.png"
                    alt="Diagram illustrating thread-safety issues with torch.compile"
                    width={1200}
                    height={675}
                    className="w-full h-auto rounded-md border border-gray-200"
                  />
                </div>
                <figcaption className="mt-2 text-sm text-gray-500 text-center">
                  Thread-safety visualization
                </figcaption>
              </figure>}

              <p className="leading-relaxed text-gray-700">
                The issue lies in the fact that the input to the model affects
                how we run our model compilation during torch.compile. For our
                purposes, we define the shape of our input as the number of
                requests x the length of the requests. The shape of our input
                matters because different input sizes determine{" "}
                <a
                  href="https://www.youtube.com/watch?v=UEdGJGz8Eyg"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  cache locality and tiling patterns
                </a>{" "}
                on the GPU which are important for fast lookups and caching
                behavior.
              </p>

              <p className="leading-relaxed text-gray-700">
                Ideally, we try to &quot;warm up&quot; the model by presenting it with
                input shapes that we would actually see in production so that
                we can cache different compiled versions of our model for the
                input shape the model sees during production. More on this{" "}
                <a
                  href="https://discuss.pytorch.org/t/does-warming-up-the-torch-compile-necessarily-trigger-all-cases/215146"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  here
                </a>
                .
              </p>

              <p className="leading-relaxed text-gray-700">
                In some cases, torch.compile can&apos;t build a compiled version of
                your model for some set of input shapes (usually for{" "}
                <a
                  href="https://pytorch.org/docs/stable/compile/programming_model.graph_breaks_index.html"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  one of these reasons
                </a>
                ). This is bad because it means that some portion of your
                model&apos;s graph cannot be optimized which leads to slowdowns. You
                know that your compiled model has graph breaks when you see a
                more than one of &quot;torch compiled regions&quot; in your profiler.
              </p>

              <p className="leading-relaxed text-gray-700">
                We had this exact problem with our custom-defined modules which
                means that we had to copy our weights to a known HuggingFace
                architecture which torch.compile worked on. Doing this led to a
                2x improvement in QPS!
              </p>

              <h2 className="text-2xl font-medium text-gray-800 mt-8 mb-2">
                Technique #3: Multiple request streams
              </h2>
              <p className="leading-relaxed text-gray-700">
                The general flow of our model was this: input request,
                tokenization, preprocessing into batches on the CPU, H2D
                transfer to GPU, CPU covers the GPU while GPU executes
                optimized torch.compile kernels, D2H transfer back to CPU, CPU
                postprocessing and result returned to client.
              </p>

              {/* TODO: Add diagram image here */}
              {<figure className="my-6">
                <div className="w-full max-w-3xl">
                  <Image
                    src="/model-flow-diagram.png"
                    alt="Diagram showing the flow of requests through the model"
                    width={1200}
                    height={675}
                    className="w-full h-auto rounded-md border border-gray-200"
                  />
                </div>
                <figcaption className="mt-2 text-sm text-gray-500 text-center">
                  Model request flow visualization
                </figcaption>
              </figure>}

              <p className="leading-relaxed text-gray-700">
                Generally by optimizing any of these steps you improve the QPS.
                However what if there was a way to not improve the latency of
                any of these steps but improve the QPS? Sounds impossible
                right?
              </p>

              <p className="leading-relaxed text-gray-700">
                Luckily for us machines nowadays have a lot of CPUs. We can
                take advantage of this by running multiple preprocessing and
                postprocessing threads at once! This is important because now
                the GPU isn&apos;t waiting for a single thread to finish
                preprocessing before it receives input for inference. By
                specifying multiple threads for preprocessing and postprocessing
                and locking the model during inference (mutex) we avoid issues
                with multi-threaded torch.compiled model inference (explained
                above).
              </p>

              <p className="leading-relaxed text-gray-700">
                My teammate incorrectly thought that parallelizing the
                preprocessing step would lead to unsafe behavior when using our
                compiled model. After correcting the misunderstanding and
                parallelizing the preprocessing steps we saw a QPS improvement
                of 1.5x!
              </p>

              <h2 className="text-2xl font-medium text-gray-800 mt-8 mb-2">
                Technique #4: Nested Jagged Tensors
              </h2>
              <p className="leading-relaxed text-gray-700">
                Usually, in production grade systems we pad our input tensors
                to some predefined <code className="bg-gray-100 px-2 py-1 rounded text-sm">max_seq_len</code> which
                makes executing torch.compile since shapes are deterministic.
                However, depending on your model you could think of using
                something called{" "}
                <a
                  href="https://pytorch.org/docs/stable/nested.html"
                  target="_blank"
                  rel="noopener noreferrer"
                  className="underline decoration-gray-400 hover:decoration-gray-600"
                >
                  NJTs (Nested Jagged Tensors)
                </a>
                . These are especially useful when most of your incoming
                requests are skewed towards shorter sequences (as was in our
                case) as most of your token_ids end up becoming pad_token_id.
                This is common across a wide variety of domains like
                recommendation systems that take user history as input,
                chatbots and speech recognition to name a few.
              </p>

              {/* TODO: Add diagram image here */}
              {<figure className="my-6">
                <div className="w-full max-w-3xl">
                  <Image
                    src="/njt-diagram.png"
                    alt="Diagram comparing padded tensors vs nested jagged tensors"
                    width={1200}
                    height={675}
                    className="w-full h-auto rounded-md border border-gray-200"
                  />
                </div>
                <figcaption className="mt-2 text-sm text-gray-500 text-center">
                  Padded tensors vs. Nested Jagged Tensors
                </figcaption>
              </figure>}

              <p className="leading-relaxed text-gray-700">
                NJTs don&apos;t come for free though. Our first attempts at making
                our model compatible with NJTs resulted in many graph breaks.
                After some debugging and collaboration with the PyTorch
                Compiler team we discovered even some operations (like
                attention) don&apos;t fully support NJTs on certain types of
                hardware (yet). This led us to converting NJTs to padded
                tensors before certain operations which slows us down but
                actually saves us from graph breaks.
              </p>

              <p className="leading-relaxed text-gray-700">
                The end result was a model that ran inference ~4x faster and in
                production led to 2.2x QPS over our initial baseline! We also
                achieved parity with our existing deployment which meant we
                were free to launch our model improvements! Yay!
              </p>

              <h2 className="text-2xl font-medium text-gray-800 mt-8 mb-2">
                Takeaways
              </h2>
              <p className="leading-relaxed text-gray-700">
                Model optimization is extremely fun because it can be
                compressed into a game that you play against the model and its
                varying input shapes. More generally, you&apos;re tring to answer the question: &quot;How far can I push QPS using my understanding of
                the underlying hardware and the different interfaces made
                available to me through PyTorch, Python and CUDA?&quot; Seeing
                immediate performance gains after testing a hypothesis is a
                feeling of excitement that feels like you solving a sudoku
                puzzle. It&apos;s a fun game to play and I highly recommend you try optimizing your models!
              </p>

              <p className="leading-relaxed text-gray-700">
                My hope is that this blog perhaps taught you a little bit more
                about how you can speed up your models and make them more
                efficient so that you can deploy artificial intelligence into
                the world :)
              </p>
            </div>
          </article>
        </div>
      </main>
    </div>
  );
}

