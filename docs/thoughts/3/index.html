<!DOCTYPE html><!--pLCfVTZ9II2gpe_cjiJ6Y--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/30d74baa196fe88a-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/throughput-pizza.png"/><link rel="stylesheet" href="/_next/static/css/e704aa8c1daf97be.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-3b0dbcae1f5cfc50.js"/><script src="/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/_next/static/chunks/964-0608b4314356e248.js" async=""></script><script src="/_next/static/chunks/main-app-33b00714f32511b2.js" async=""></script><script src="/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/_next/static/chunks/63-b8ba2254fb9ac97a.js" async=""></script><script src="/_next/static/chunks/app/thoughts/3/page-9902c2776df21ef6.js" async=""></script><meta name="next-size-adjust" content=""/><title>James Moore</title><meta name="description" content="James Moore&#x27;s personal website"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_188709 __variable_9a8899 __variable_81f11e antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-green-50 flex flex-col"><header class="fixed top-0 left-0 right-0 bg-green-50 border-b border-black z-50 py-4 md:py-8 px-4 md:px-8"><div class="flex justify-between items-center"><a class="font-noto-serif text-sm md:text-base text-gray-700 hover:text-gray-900 transition-colors relative group" href="/">jmoore.info<span class="absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full"></span></a><nav><ul class="flex space-x-3 sm:space-x-4 md:space-x-8 font-noto-serif text-gray-700 text-xs sm:text-sm md:text-base"><li><a class="relative group transition-colors hover:text-gray-900" href="/work/">work<span class="absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full"></span></a></li><li><a class="relative group transition-colors hover:text-gray-900" href="/common-book/">commonplace book<span class="absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full"></span></a></li><li><a class="relative group transition-colors hover:text-gray-900" href="/thoughts/">thoughts<span class="absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full"></span></a></li></ul></nav></div></header><main class="flex-1 pt-32 pb-16 px-8 max-w-4xl"><div class="font-noto-serif text-gray-800 space-y-8"><a class="text-gray-600 hover:text-gray-800 transition-colors text-sm" href="/thoughts/">← back to thoughts</a><article class="space-y-8"><header class="space-y-4"><h1 class="text-4xl font-medium text-gray-800">How to make models run faster</h1><time class="text-sm text-gray-500 italic block">December 2025</time></header><div class="prose prose-gray max-w-none space-y-6"><p class="leading-relaxed text-gray-700">As part of my work at Meta I was given the (sorta) vague task of &quot;improve this language identification model please.&quot; at first i was pretty overwhelmed since I had never touched a production model let alone a model that was queried as much as this one across the company but (as any over-ambitious new grad) I was determined to make it happen. Long story short i was able to fix some pretty serious problems regarding english recall with the model which basically meant that obviously english text wasn&#x27;t being classified as some other language anymore which is really good!</p><p class="leading-relaxed text-gray-700">However there was another issue and one that I think is probably equally as important as fixing critical model quality issues. How do we actually transfer these quality improvements to production? As is normal at Meta we were deploying on a new internal framework so we were completely unaware of any potential challenges that we could face. We sort of thought that since our improved model had the roughly the same number of parameters and architecture as our previous model that we would automatically have the same throughput in production. We were wrong about that (lol). We had roughly 2.5x throughput than our previous deployment which meant that we couldn&#x27;t deploy our improvements at all!</p><p class="leading-relaxed text-gray-700">If you don&#x27;t know what throughput means, what determines it and why it&#x27;s super-duper important to benchmarking performance here&#x27;s a quick explanation:</p><p class="leading-relaxed text-gray-700">Imagine your waiting in some line for pizza. Now what would you expect that the time it takes to go from entering the store to actually getting the slice of pepperoni pizza (the best kind of pizza) would depends on? Hmmm… you&#x27;d probably guess it depends on the number of people in line, the number of cashiers taking orders, how long each order takes, the number of chefs the number of ovens, and then generally how fast each of the steps take.</p><p class="leading-relaxed text-gray-700">We can define throughput (in this context) as the number of people that the pizza shop can serve over a given interval of time. We can also define E2E (end to end) latency as the time it takes for you to enter the shop to actually getting the pizza. Here&#x27;s a neat diagram generated by NanoBanana pro which probably does a better job of describing the concept than I just did:</p><figure class="my-6"><div class="w-full max-w-3xl"><img alt="Diagram illustrating throughput and E2E latency concepts" width="1200" height="675" decoding="async" data-nimg="1" class="w-full h-auto rounded-md border border-gray-200" style="color:transparent" src="/throughput-pizza.png"/></div><figcaption class="mt-2 text-sm text-gray-500 text-center">Throughput and latency visualization</figcaption></figure><p class="leading-relaxed text-gray-700">So to translate, instead of a pizza shop we&#x27;re dealing with production-grade machine learning systems using thousands of really expensive GPUs (aka the chefs) and trillions of requests (pizza orders aka HTTP request) and seeing how many requests we can process successfully over a period of around a second a.k.a QPS (queries per second).</p><p class="leading-relaxed text-gray-700">Cool, back to the story. So now we&#x27;re panicking because we can&#x27;t actually deploy our model since we&#x27;re regressing compared to our old model deployment. This is pretty bad since a lot of our clients don&#x27;t have extra compute lying around to spend on a more &quot;expensive&quot; new model and thus we can&#x27;t actually launch our improvements!</p><p class="leading-relaxed text-gray-700">This is where I was tasked with trying to improve the QPS of our model back to where it was previously…. Let&#x27;s do some model optimization to save the day:</p><h2 class="text-2xl font-medium text-gray-800 mt-8 mb-2">Technique #1: profiling!</h2><p class="leading-relaxed text-gray-700">How can we optimize something that we can&#x27;t even see. It would be really nice to track where a single request goes in order to understand the different transformations it undergoes until it comes out the other end of our system as a language prediction! This is where profiling our CPU and GPU is really useful. The profiler is something that you run as you&#x27;re running a QPS (load) test on a local GPU so you can see the actual activity that&#x27;s happening. Understanding how to use the profiler is super important to improving your model. Here&#x27;s an example of a what a profiler looks like:</p><figure class="my-6"><div class="w-full max-w-3xl"><img alt="Perfetto (a popular profiling tool) GPU trace" loading="lazy" width="1200" height="675" decoding="async" data-nimg="1" class="w-full h-auto rounded-md border border-gray-200" style="color:transparent" src="/gpu-trace.png"/></div><figcaption class="mt-2 text-sm text-gray-500 text-center"><a href="https://modal.com/docs/examples/torch_profiling" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">Perfetto (a popular profiling tool)</a> <!-- -->GPU trace</figcaption></figure><p class="leading-relaxed text-gray-700">What profiling is most useful for is understanding what is bottlenecking your system which can be used to generate hypotheses about what can improve your system&#x27;s QPS the most. Thus, we can reduce model optimization efforts down to this (super generalized) iterative loop:</p><ol class="list-decimal list-inside space-y-2 leading-relaxed text-gray-700 ml-4"><li>Profile model on standalone GPU during QPS load test</li><li>Read profiler output to identify bottlenecks</li><li>Optimize model deployment implementation</li><li>Reprofile model and measure QPS</li><li>If QPS increases that&#x27;s good!! and if not… try again :)</li></ol><p class="leading-relaxed text-gray-700">If you want more details on GPU profiling I suggest you read this<!-- --> <a href="https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">PyTorch documentation</a> <!-- -->as it goes in depth and is written by people that are better than me at this stuff :)</p><h2 class="text-2xl font-medium text-gray-800 mt-8 mb-2">Technique #2: torch.compile</h2><p class="leading-relaxed text-gray-700">One powerful model optimization technique is the idea of &quot;compiling&quot; your model. When I first heard of this I had only heard &quot;compilation&quot; in the context of programming languages. My memories flashed-back to how compilers do things like pruning syntax trees, unrolling loops and removing unnecessary branching so that the code we write is optimized to run on the machine we&#x27;re using. In essence, torch.compile is trying to do the same thing except that the &quot;code&quot; is a PyTorch model defined by individual and nested modules and the machines we&#x27;re running on are (usually) GPUs. This results in a model that has equivalent output but runs much faster.</p><p class="leading-relaxed text-gray-700">Now how exactly does this happen and how do we do this? From a client&#x27;s perspective it&#x27;s easy! You just define some PyTorch model and then call:</p><pre class="bg-gray-100 p-4 rounded-md border border-gray-200 overflow-x-auto"><code class="text-sm text-gray-800">compiled_model = torch.compile(model)</code></pre><p class="leading-relaxed text-gray-700">Usually this works for most models since the actual operations which define most machine learning models are shared across various types of models. torch.compile is really good at spotting and taking advantage of these redundancies and (generally) replaces multiple instructions with kernels that are written to optimize the execution of those instructions on specific machines. How exactly to identify certain redundancies across models and write these optimized kernels is a (very difficult) problem that very talented people try to solve! More about how model compilation works is<!-- --> <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">here</a>.</p><p class="leading-relaxed text-gray-700">One quirk about using torch.compile is that it is NOT thread-safe. Why is this an issue? Going back to the pizza analogy now:</p><p class="leading-relaxed text-gray-700">Imagine the Head Chef (the compiler) invents a brand new, super-fast way to make pizza and writes it in a master recipe book. Now, two chefs receiving different orders (pepperoni and vegetarian pizza) try to both optimize the recipe for their different orders. As one chef scribbles a change for a new order, the other chef unknowingly follows the halfway-written instruction and ruins their pizza.</p><figure class="my-6"><div class="w-full max-w-3xl"><img alt="Diagram illustrating thread-safety issues with torch.compile" loading="lazy" width="1200" height="675" decoding="async" data-nimg="1" class="w-full h-auto rounded-md border border-gray-200" style="color:transparent" src="/torch-compile-pizza.png"/></div><figcaption class="mt-2 text-sm text-gray-500 text-center">Thread-safety visualization</figcaption></figure><p class="leading-relaxed text-gray-700">The issue lies in the fact that the input to the model affects how we run our model compilation during torch.compile. For our purposes, we define the shape of our input as the number of requests x the length of the requests. The shape of our input matters because different input sizes determine<!-- --> <a href="https://www.youtube.com/watch?v=UEdGJGz8Eyg" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">cache locality and tiling patterns</a> <!-- -->on the GPU which are important for fast lookups and caching behavior.</p><p class="leading-relaxed text-gray-700">Ideally, we try to &quot;warm up&quot; the model by presenting it with input shapes that we would actually see in production so that we can cache different compiled versions of our model for the input shape the model sees during production. More on this<!-- --> <a href="https://discuss.pytorch.org/t/does-warming-up-the-torch-compile-necessarily-trigger-all-cases/215146" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">here</a>.</p><p class="leading-relaxed text-gray-700">In some cases, torch.compile can&#x27;t build a compiled version of your model for some set of input shapes (usually for<!-- --> <a href="https://pytorch.org/docs/stable/compile/programming_model.graph_breaks_index.html" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">one of these reasons</a>). This is bad because it means that some portion of your model&#x27;s graph cannot be optimized which leads to slowdowns. You know that your compiled model has graph breaks when you see a more than one of &quot;torch compiled regions&quot; in your profiler.</p><p class="leading-relaxed text-gray-700">We had this exact problem with our custom-defined modules which means that we had to copy our weights to a known HuggingFace architecture which torch.compile worked on. Doing this led to a 2x improvement in QPS!</p><h2 class="text-2xl font-medium text-gray-800 mt-8 mb-2">Technique #3: Multiple request streams</h2><p class="leading-relaxed text-gray-700">The general flow of our model was this: input request, tokenization, preprocessing into batches on the CPU, H2D transfer to GPU, CPU covers the GPU while GPU executes optimized torch.compile kernels, D2H transfer back to CPU, CPU postprocessing and result returned to client.</p><figure class="my-6"><div class="w-full max-w-3xl"><img alt="Diagram showing the flow of requests through the model" loading="lazy" width="1200" height="675" decoding="async" data-nimg="1" class="w-full h-auto rounded-md border border-gray-200" style="color:transparent" src="/model-flow-diagram.png"/></div><figcaption class="mt-2 text-sm text-gray-500 text-center">Model request flow visualization</figcaption></figure><p class="leading-relaxed text-gray-700">Generally by optimizing any of these steps you improve the QPS. However what if there was a way to not improve the latency of any of these steps but improve the QPS? Sounds impossible right?</p><p class="leading-relaxed text-gray-700">Luckily for us machines nowadays have a lot of CPUs. We can take advantage of this by running multiple preprocessing and postprocessing threads at once! This is important because now the GPU isn&#x27;t waiting for a single thread to finish preprocessing before it receives input for inference. By specifying multiple threads for preprocessing and postprocessing and locking the model during inference (mutex) we avoid issues with multi-threaded torch.compiled model inference (explained above).</p><p class="leading-relaxed text-gray-700">My teammate incorrectly thought that parallelizing the preprocessing step would lead to unsafe behavior when using our compiled model. After correcting the misunderstanding and parallelizing the preprocessing steps we saw a QPS improvement of 1.5x!</p><h2 class="text-2xl font-medium text-gray-800 mt-8 mb-2">Technique #4: Nested Jagged Tensors</h2><p class="leading-relaxed text-gray-700">Usually, in production grade systems we pad our input tensors to some predefined <code class="bg-gray-100 px-2 py-1 rounded text-sm">max_seq_len</code> which makes executing torch.compile since shapes are deterministic. However, depending on your model you could think of using something called<!-- --> <a href="https://pytorch.org/docs/stable/nested.html" target="_blank" rel="noopener noreferrer" class="underline decoration-gray-400 hover:decoration-gray-600">NJTs (Nested Jagged Tensors)</a>. These are especially useful when most of your incoming requests are skewed towards shorter sequences (as was in our case) as most of your token_ids end up becoming pad_token_id. This is common across a wide variety of domains like recommendation systems that take user history as input, chatbots and speech recognition to name a few.</p><figure class="my-6"><div class="w-full max-w-3xl"><img alt="Diagram comparing padded tensors vs nested jagged tensors" loading="lazy" width="1200" height="675" decoding="async" data-nimg="1" class="w-full h-auto rounded-md border border-gray-200" style="color:transparent" src="/njt-diagram.png"/></div><figcaption class="mt-2 text-sm text-gray-500 text-center">Padded tensors vs. Nested Jagged Tensors</figcaption></figure><p class="leading-relaxed text-gray-700">NJTs don&#x27;t come for free though. Our first attempts at making our model compatible with NJTs resulted in many graph breaks. After some debugging and collaboration with the PyTorch Compiler team we discovered even some operations (like attention) don&#x27;t fully support NJTs on certain types of hardware (yet). This led us to converting NJTs to padded tensors before certain operations which slows us down but actually saves us from graph breaks.</p><p class="leading-relaxed text-gray-700">The end result was a model that ran inference ~4x faster and in production led to 2.2x QPS over our initial baseline! We also achieved parity with our existing deployment which meant we were free to launch our model improvements! Yay!</p><h2 class="text-2xl font-medium text-gray-800 mt-8 mb-2">Takeaways</h2><p class="leading-relaxed text-gray-700">Model optimization is extremely fun because it can be compressed into a game that you play against the model and its varying input shapes. More generally, you&#x27;re tring to answer the question: &quot;How far can I push QPS using my understanding of the underlying hardware and the different interfaces made available to me through PyTorch, Python and CUDA?&quot; Seeing immediate performance gains after testing a hypothesis is a feeling of excitement that feels like you solving a sudoku puzzle. It&#x27;s a fun game to play and I highly recommend you try optimizing your models!</p><p class="leading-relaxed text-gray-700">My hope is that this blog perhaps taught you a little bit more about how you can speed up your models and make them more efficient so that you can deploy artificial intelligence into the world :)</p></div></article></div></main></div><!--$--><!--/$--><script src="/_next/static/chunks/webpack-3b0dbcae1f5cfc50.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"63\",\"static/chunks/63-b8ba2254fb9ac97a.js\",\"643\",\"static/chunks/app/thoughts/3/page-9902c2776df21ef6.js\"],\"\"]\na:I[8393,[],\"\"]\n:HL[\"/_next/static/media/30d74baa196fe88a-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/e704aa8c1daf97be.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"pLCfVTZ9II2gpe-cjiJ6Y\",\"p\":\"\",\"c\":[\"\",\"thoughts\",\"3\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"thoughts\",{\"children\":[\"3\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e704aa8c1daf97be.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_188709 __variable_9a8899 __variable_81f11e antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"thoughts\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"3\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-green-50 flex flex-col\",\"children\":[[\"$\",\"header\",null,{\"className\":\"fixed top-0 left-0 right-0 bg-green-50 border-b border-black z-50 py-4 md:py-8 px-4 md:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center\",\"children\":[[\"$\",\"$L4\",null,{\"href\":\"/\",\"className\":\"font-noto-serif text-sm md:text-base text-gray-700 hover:text-gray-900 transition-colors relative group\",\"children\":[\"jmoore.info\",[\"$\",\"span\",null,{\"className\":\"absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full\"}]]}],[\"$\",\"nav\",null,{\"children\":[\"$\",\"ul\",null,{\"className\":\"flex space-x-3 sm:space-x-4 md:space-x-8 font-noto-serif text-gray-700 text-xs sm:text-sm md:text-base\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L4\",null,{\"href\":\"/work\",\"className\":\"relative group transition-colors hover:text-gray-900\",\"children\":[\"work\",[\"$\",\"span\",null,{\"className\":\"absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L4\",null,{\"href\":\"/common-book\",\"className\":\"relative group transition-colors hover:text-gray-900\",\"children\":[\"commonplace book\",[\"$\",\"span\",null,{\"className\":\"absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L4\",null,{\"href\":\"/thoughts\",\"className\":\"relative group transition-colors hover:text-gray-900\",\"children\":[\"thoughts\",[\"$\",\"span\",null,{\"className\":\"absolute left-0 bottom-0 w-0 h-0.5 bg-gray-700 transition-all duration-300 group-hover:w-full\"}]]}]}]]}]}]]}]}],[\"$\",\"main\",null,{\"className\":\"flex-1 pt-32 pb-16 px-8 max-w-4xl\",\"children\":[\"$\",\"div\",null,{\"className\":\"font-noto-serif text-gray-800 space-y-8\",\"children\":[[\"$\",\"$L4\",null,{\"href\":\"/thoughts\",\"className\":\"text-gray-600 hover:text-gray-800 transition-colors text-sm\",\"children\":\"← back to thoughts\"}],[\"$\",\"article\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"header\",null,{\"className\":\"space-y-4\",\"children\":[\"$L5\",\"$L6\"]}],\"$L7\"]}]]}]}]]}],null,\"$L8\"]}],{},null,false]},null,false]},null,false]},null,false],\"$L9\",false]],\"m\":\"$undefined\",\"G\":[\"$a\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"b:I[3063,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"63\",\"static/chunks/63-b8ba2254fb9ac97a.js\",\"643\",\"static/chunks/app/thoughts/3/page-9902c2776df21ef6.js\"],\"Image\"]\n2e:I[9665,[],\"OutletBoundary\"]\n30:I[4911,[],\"AsyncMetadataOutlet\"]\n32:I[9665,[],\"ViewportBoundary\"]\n34:I[9665,[],\"MetadataBoundary\"]\n35:\"$Sreact.suspense\"\n5:[\"$\",\"h1\",null,{\"className\":\"text-4xl font-medium text-gray-800\",\"children\":\"How to make models run faster\"}]\n6:[\"$\",\"time\",null,{\"className\":\"text-sm text-gray-500 italic block\",\"children\":\"December 2025\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"prose prose-gray max-w-none space-y-6\",\"children\":[[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"As part of my work at Meta I was given the (sorta) vague task of \\\"improve this language identification model please.\\\" at first i was pretty overwhelmed since I had never touched a production model let alone a model that was queried as much as this one across the company but (as any over-ambitious new grad) I was determined to make it happen. Long story short i was able to fix some pretty serious problems regarding english recall with the model which basically meant that obviously english text wasn't being classified as some other language anymore which is really good!\"}],[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"However there was another issue and one that I think is probably equally as important as fixing critical model quality issues. How do we actually transfer these quality improvements to production? As is normal at Meta we were deploying on a new internal framework so we were completely unaware of any potential challenges that we could face. We sort of thought that since our improved model had the roughly the same number of parameters and architecture as our previous model that we would automatically have the same throughput in production. We were wrong about that (lol). We had roughly 2.5x throughput than our previous deployment which meant that we couldn't deploy our improvements at all!\"}],[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"If you don't know what throughput means, what determines it and why it's super-duper important to benchmarking performance here's a quick explanation:\"}],[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Imagine your waiting in some line for pizza. Now what would you expect that the time it takes to go from entering the store to actually getting the slice of pepperoni pizza (the best kind of pizza) would depends on? Hmmm… you'd probably guess it depends on the number of people in line, the number of cashiers taking orders, how long each order takes, the number of chefs the number of ovens, and then generally how fast each of the steps take.\"}],[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"We can define throughput (in this context) as the number of people that the pizza shop can serve over a given interval of time. We can also define E2E (end to end) latency as the time it takes for you to enter the shop to actually getting the pizza. Here's a neat diagram generated by NanoBanana pro which probably does a better job of describing the concept than I just did:\"}],[\"$\",\"figure\",null,{\"className\":\"my-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl\",\"children\":[\"$\",\"$Lb\",null,{\"src\":\"/throughput-pizza.png\",\"alt\":\"Diagram illustrating throughput and E2E latency concepts\",\"width\":1200,\"height\":675,\"className\":\"w-full h-auto rounded-md border border-gray-200\",\"priority\":true}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-sm text-gray-500 text-center\",\"children\":\"Throughput and latency visualization\"}]]}],[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"So to translate, instead of a pizza shop we're dealing with production-grade machine learning systems using thousands of really expensive GPUs (aka the chefs) and trillions of requests (pizza orders aka HTTP request) and seeing how many requests we can process successfully over a period of around a second a.k.a QPS (queries per second).\"}],\"$Lc\",\"$Ld\",\"$Le\",\"$Lf\",\"$L10\",\"$L11\",\"$L12\",\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\",\"$L1b\",\"$L1c\",\"$L1d\",\"$L1e\",\"$L1f\",\"$L20\",\"$L21\",\"$L22\",\"$L23\",\"$L24\",\"$L25\",\"$L26\",\"$L27\",\"$L28\",\"$L29\",\"$L2a\",\"$L2b\",\"$L2c\",\"$L2d\"]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"$L2e\",null,{\"children\":[\"$L2f\",[\"$\",\"$L30\",null,{\"promise\":\"$@31\"}]]}]\n9:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L32\",null,{\"children\":\"$L33\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L34\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$35\",null,{\"fallback\":null,\"children\":\"$L36\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Cool, back to the story. So now we're panicking because we can't actually deploy our model since we're regressing compared to our old model deployment. This is pretty bad since a lot of our clients don't have extra compute lying around to spend on a more \\\"expensive\\\" new model and thus we can't actually launch our improvements!\"}]\nd:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"This is where I was tasked with trying to improve the QPS of our model back to where it was previously…. Let's do some model optimization to save the day:\"}]\ne:[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium text-gray-800 mt-8 mb-2\",\"children\":\"Technique #1: profiling!\"}]\nf:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"How can we optimize something that we can't even see. It would be really nice to track where a single request goes in order to understand the different transformations it undergoes until it comes out the other end of our system as a language prediction! This is where profiling our CPU and GPU is really useful. The profiler is something that you run as you're running a QPS (load) test on a local GPU so you can see the actual activity that's happening. Understanding how to use the profiler is super important to improving your model. Here's an example of a what a profiler looks like:\"}]\n10:[\"$\",\"figure\",null,{\"className\":\"my-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl\",\"children\":[\"$\",\"$Lb\",null,{\"src\":\"/gpu-trace.png\",\"alt\":\"Perfetto (a popular profiling tool) GPU trace\",\"width\":1200,\"height\":675,\"className\":\"w-full h-auto rounded-md border border-gray-200\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-sm text-gray-500 text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://modal.com/docs/examples/torch_profiling\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"Perfetto (a popular profiling tool)\"}],"])</script><script>self.__next_f.push([1,"\" \",\"GPU trace\"]}]]}]\n11:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"What profiling is most useful for is understanding what is bottlenecking your system which can be used to generate hypotheses about what can improve your system's QPS the most. Thus, we can reduce model optimization efforts down to this (super generalized) iterative loop:\"}]\n12:[\"$\",\"ol\",null,{\"className\":\"list-decimal list-inside space-y-2 leading-relaxed text-gray-700 ml-4\",\"children\":[[\"$\",\"li\",null,{\"children\":\"Profile model on standalone GPU during QPS load test\"}],[\"$\",\"li\",null,{\"children\":\"Read profiler output to identify bottlenecks\"}],[\"$\",\"li\",null,{\"children\":\"Optimize model deployment implementation\"}],[\"$\",\"li\",null,{\"children\":\"Reprofile model and measure QPS\"}],[\"$\",\"li\",null,{\"children\":\"If QPS increases that's good!! and if not… try again :)\"}]]}]\n13:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"If you want more details on GPU profiling I suggest you read this\",\" \",[\"$\",\"a\",null,{\"href\":\"https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"PyTorch documentation\"}],\" \",\"as it goes in depth and is written by people that are better than me at this stuff :)\"]}]\n14:[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium text-gray-800 mt-8 mb-2\",\"children\":\"Technique #2: torch.compile\"}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"One powerful model optimization technique is the idea of \\\"compiling\\\" your model. When I first heard of this I had only heard \\\"compilation\\\" in the context of programming languages. My memories flashed-back to how compilers do things like pruning syntax trees, unrolling loops and removing unnecessary branching so that the code we write is optimized to run on the machine we're using. In essence, torch.compile is trying to do the same thing except that the \\\"code\\\" is a PyTorch model defined by individual and nested modules and the machines we're running on are (usually) GPUs. This results in a model that has equivalent output but runs much faster.\"}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Now how exactly does this happen and how do we do this? From a client's perspective it's easy! You just define some PyTorch model and then call:\"}]\n17:[\"$\",\"pre\",null,{\"className\":\"bg-gray-100 p-4 rounded-md border border-gray-200 overflow-x-auto\",\"children\":[\"$\",\"code\",null,{\"className\":\"text-sm text-gray-800\",\"children\":\"compiled_model = torch.compile(model)\"}]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"Usually this works for most models since the actual operations which define most machine learning models are shared across various types of models. torch.compile is really good at spotting and taking advantage of these redundancies and (generally) replaces multiple instructions with kernels that are written to optimize the execution of those instructions on specific machines. How exactly to identify certain redundancies across models and write these optimized kernels is a (very difficult) problem that very talented people try to solve! More about how model compilation works is\",\" \",[\"$\",\"a\",null,{\"href\":\"https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"here\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"One quirk about using torch.compile is that it is NOT thread-safe. Why is this an issue? Going back to the pizza analogy now:\"}]\n1a:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Imagine the Head Chef (the compiler) invents a brand new, super-fast way to make pizza and writes it in a master recipe book. Now, two chefs receiving different orders (pepperoni and vegetarian pizza) try to both optimize the recipe for their different orders. As one chef scribbles a change for a new order, the other chef unknowingly follows the halfway-written instruction and ruins their pizza.\"}]\n1b:[\"$\",\"figure\",null,{\"className\":\"my-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl\",\"children\":[\"$\",\"$Lb\",null,{\"src\":\"/torch-compile-pizza.png\",\"alt\":\"Diagram illustrating thread-safety issues with torch.compile\",\"width\":1200,\"height\":675,\"className\":\"w-full h-auto rounded-md border border-gray-200\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-sm text-gray-500 text-center\",\"children\":\"Thread-safety visualization\"}]]}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"The issue lies in the fact that the input to the model affects how we run our model compilation during torch.compile. For our purposes, we define the shape of our input as the number of requests x the length of the requests. The shape of our input matters because different input sizes determine\",\" \",[\"$\",\"a\",null,{\"href\":\"https://www.youtube.com/watch?v=UEdGJGz8Eyg\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"cache locality and tiling patterns\"}],\" \",\"on the GPU which are important for fast lookups and caching behavior.\"]}]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"Ideally, we try to \\\"warm up\\\" the model by presenting it with input shapes that we would actually see in production so that we can cache different compiled versions of our model for the input shape the model sees during production. More on this\",\" \",[\"$\",\"a\",null,{\"href\":\"https://discuss.pytorch.org/t/does-warming-up-the-torch-compile-necessarily-trigger-all-cases/215146\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"here\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"In some cases, torch.compile can't build a compiled version of your model for some set of input shapes (usually for\",\" \",[\"$\",\"a\",null,{\"href\":\"https://pytorch.org/docs/stable/compile/programming_model.graph_breaks_index.html\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"one of these reasons\"}],\"). This is bad because it means that some portion of your model's graph cannot be optimized which leads to slowdowns. You know that your compiled model has graph breaks when you see a more than one of \\\"torch compiled regions\\\" in your profiler.\"]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"We had this exact problem with our custom-defined modules which means that we had to copy our weights to a known HuggingFace architecture which torch.compile worked on. Doing this led to a 2x improvement in QPS!\"}]\n20:[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium text-gray-800 mt-8 mb-2\",\"children\":\"Technique #3: Multiple request streams\"}]\n21:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"The general flow of our model was this: input request, tokenization, preprocessing into batches on the CPU, H2D transfer to GPU, CPU covers the GPU while GPU executes optimized torch.compile kernels, D2H transfer back to CPU, CPU postprocessing and result returned to client.\"}]\n22:[\"$\",\"figure\",null,{\"className\":\"my-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl\",\"children\":[\"$\",\"$Lb\",null,{\"src\":\"/model-flow-diagram.png\",\"alt\":\"Diagram showing the flow of requests through the model\",\"width\":1200,\"height\":675,\"className\":\"w-full h-auto rounded-md border border-gray-200\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-sm text-gray-500 text-center\",\"children\":\"Model request flow visualization\"}]]}]\n23:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Generally by optimizing any of these steps you improve the QPS. However what if there was a way to not improve the latency of any of these steps but improve the QPS? Sounds impossible right?\"}]\n24:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Luckily for us machines nowadays have a lot of CPUs. We can take advantage of this by running multiple preprocessing and postprocessing threads at once! This is important because now the GPU isn't waiting for a single thread to finish preprocessing before it receives input for inference. By specifying multiple threads for preprocessing and postprocessing and locking the model during inference (mutex) we avoid issues with multi-threaded torch.compiled model inference (explained above"])</script><script>self.__next_f.push([1,").\"}]\n25:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"My teammate incorrectly thought that parallelizing the preprocessing step would lead to unsafe behavior when using our compiled model. After correcting the misunderstanding and parallelizing the preprocessing steps we saw a QPS improvement of 1.5x!\"}]\n26:[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium text-gray-800 mt-8 mb-2\",\"children\":\"Technique #4: Nested Jagged Tensors\"}]\n"])</script><script>self.__next_f.push([1,"27:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":[\"Usually, in production grade systems we pad our input tensors to some predefined \",[\"$\",\"code\",null,{\"className\":\"bg-gray-100 px-2 py-1 rounded text-sm\",\"children\":\"max_seq_len\"}],\" which makes executing torch.compile since shapes are deterministic. However, depending on your model you could think of using something called\",\" \",[\"$\",\"a\",null,{\"href\":\"https://pytorch.org/docs/stable/nested.html\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"underline decoration-gray-400 hover:decoration-gray-600\",\"children\":\"NJTs (Nested Jagged Tensors)\"}],\". These are especially useful when most of your incoming requests are skewed towards shorter sequences (as was in our case) as most of your token_ids end up becoming pad_token_id. This is common across a wide variety of domains like recommendation systems that take user history as input, chatbots and speech recognition to name a few.\"]}]\n"])</script><script>self.__next_f.push([1,"28:[\"$\",\"figure\",null,{\"className\":\"my-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl\",\"children\":[\"$\",\"$Lb\",null,{\"src\":\"/njt-diagram.png\",\"alt\":\"Diagram comparing padded tensors vs nested jagged tensors\",\"width\":1200,\"height\":675,\"className\":\"w-full h-auto rounded-md border border-gray-200\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-sm text-gray-500 text-center\",\"children\":\"Padded tensors vs. Nested Jagged Tensors\"}]]}]\n29:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"NJTs don't come for free though. Our first attempts at making our model compatible with NJTs resulted in many graph breaks. After some debugging and collaboration with the PyTorch Compiler team we discovered even some operations (like attention) don't fully support NJTs on certain types of hardware (yet). This led us to converting NJTs to padded tensors before certain operations which slows us down but actually saves us from graph breaks.\"}]\n2a:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"The end result was a model that ran inference ~4x faster and in production led to 2.2x QPS over our initial baseline! We also achieved parity with our existing deployment which meant we were free to launch our model improvements! Yay!\"}]\n2b:[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium text-gray-800 mt-8 mb-2\",\"children\":\"Takeaways\"}]\n2c:[\"$\",\"p\",null,{\"className\":\"leading-relaxed text-gray-700\",\"children\":\"Model optimization is extremely fun because it can be compressed into a game that you play against the model and its varying input shapes. More generally, you're tring to answer the question: \\\"How far can I push QPS using my understanding of the underlying hardware and the different interfaces made available to me through PyTorch, Python and CUDA?\\\" Seeing immediate performance gains after testing a hypothesis is a feeling of excitement that feels like you solving a sudoku puzzle. It's a fun game to play and I highly recommend you try optimizing your models!\"}]\n2d:[\"$\",\"p\",null,{\"cl"])</script><script>self.__next_f.push([1,"assName\":\"leading-relaxed text-gray-700\",\"children\":\"My hope is that this blog perhaps taught you a little bit more about how you can speed up your models and make them more efficient so that you can deploy artificial intelligence into the world :)\"}]\n"])</script><script>self.__next_f.push([1,"33:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n2f:null\n"])</script><script>self.__next_f.push([1,"31:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"James Moore\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"James Moore's personal website\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"36:\"$31:metadata\"\n"])</script></body></html>